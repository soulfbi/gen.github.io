{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eea8652f-089d-42c0-bca9-d65c6531d3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Value Function (4x4 Grid):\n",
      "\n",
      "[[  0.    -6.88 -10.08 -12.16]\n",
      " [ -7.72  -8.55  -9.21  -9.92]\n",
      " [-10.24  -9.13  -8.32  -7.02]\n",
      " [-11.97  -9.94  -7.31   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "**1.Policy evaluation using Monte Carlo Method (using any method for 4×4 grid world)**\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "np.random.seed(0)\n",
    "\n",
    "# -------- GRID SIZE --------\n",
    "rows = 4\n",
    "cols = 4\n",
    "n = rows * cols\n",
    "\n",
    "# terminal states\n",
    "terminal = [0, n-1]\n",
    "\n",
    "# actions = L, R, U, D\n",
    "actions = [(0,-1),(0,1),(-1,0),(1,0)]\n",
    "\n",
    "def step(s, a):\n",
    "    r, c = divmod(s, cols)\n",
    "    dr, dc = a\n",
    "    nr, nc = r + dr, c + dc\n",
    "\n",
    "    # invalid move → stay in same state\n",
    "    if nr < 0 or nr >= rows or nc < 0 or nc >= cols:\n",
    "        nr, nc = r, c\n",
    "\n",
    "    ns = nr * cols + nc\n",
    "    return ns, -1      # reward = -1\n",
    "\n",
    "\n",
    "# -------- RANDOM POLICY (π(a|s) = 1/4) --------\n",
    "def policy(s):\n",
    "    return actions[np.random.randint(4)]\n",
    "\n",
    "\n",
    "# -------- MONTE CARLO POLICY EVALUATION --------\n",
    "V = defaultdict(float)\n",
    "returns = defaultdict(list)\n",
    "\n",
    "episodes = 5000\n",
    "\n",
    "for _ in range(episodes):\n",
    "\n",
    "    # start from a random NON-terminal state\n",
    "    s = np.random.randint(1, n-1)\n",
    "    while s in terminal:\n",
    "        s = np.random.randint(1, n-1)\n",
    "\n",
    "    episode = []\n",
    "\n",
    "    # generate full episode\n",
    "    while True:\n",
    "        a = policy(s)\n",
    "        ns, r = step(s, a)\n",
    "        episode.append((s, r))\n",
    "\n",
    "        if ns in terminal:\n",
    "            break\n",
    "        s = ns\n",
    "\n",
    "    # FIRST-VISIT MC UPDATE\n",
    "    G = 0\n",
    "    visited = set()\n",
    "\n",
    "    for s, r in reversed(episode):\n",
    "        G += r\n",
    "        if s not in visited:\n",
    "            visited.add(s)\n",
    "            returns[s].append(G)\n",
    "            V[s] = np.mean(returns[s])\n",
    "\n",
    "\n",
    "# -------- PRINT VALUE FUNCTION --------\n",
    "value_grid = np.zeros((rows, cols))\n",
    "for s, v in V.items():\n",
    "    value_grid[s // cols][s % cols] = round(v, 2)\n",
    "\n",
    "print(\"Monte Carlo Value Function (4x4 Grid):\\n\")\n",
    "print(value_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d3d4cc9-6ea8-4259-b151-f0e22cad43f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 4 iterations\n",
      "\n",
      "Optimal Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "Optimal Policy (as arrows):\n",
      "[['T' '←' '←' '↓']\n",
      " ['↑' '↑' '↑' '↓']\n",
      " ['↑' '↑' '↓' '↓']\n",
      " ['↑' '→' '→' 'T']]\n"
     ]
    }
   ],
   "source": [
    "#Q2: Simple MDP simulation in a 4×4 grid world for Markov Decision Process (MDP) for planning with Value Iteration\n",
    "import numpy as np\n",
    "\n",
    "GRID_SIZE = 4\n",
    "STATE_COUNT = GRID_SIZE * GRID_SIZE\n",
    "TERMINAL_STATES = [0, STATE_COUNT - 1]\n",
    "ACTIONS = [\"U\", \"D\", \"L\", \"R\"]\n",
    "GAMMA = 1.0\n",
    "THRESHOLD = 1e-4\n",
    "\n",
    "V = np.zeros(STATE_COUNT)\n",
    "\n",
    "def state_to_pos(state):\n",
    "    return state // GRID_SIZE, state % GRID_SIZE\n",
    "\n",
    "def pos_to_state(row, col):\n",
    "    return row * GRID_SIZE + col\n",
    "\n",
    "def step(state, action):\n",
    "    if state in TERMINAL_STATES:\n",
    "        return state, 0\n",
    "\n",
    "    r, c = state_to_pos(state)\n",
    "\n",
    "    if action == \"U\": r = max(r - 1, 0)\n",
    "    elif action == \"D\": r = min(r + 1, GRID_SIZE - 1)\n",
    "    elif action == \"L\": c = max(c - 1, 0)\n",
    "    elif action == \"R\": c = min(c + 1, GRID_SIZE - 1)\n",
    "\n",
    "    next_state = pos_to_state(r, c)\n",
    "    reward = -1\n",
    "    return next_state, reward\n",
    "\n",
    "iteration = 0\n",
    "while True:\n",
    "    delta = 0\n",
    "    new_V = np.copy(V)\n",
    "\n",
    "    for s in range(STATE_COUNT):\n",
    "        if s in TERMINAL_STATES:\n",
    "            continue\n",
    "\n",
    "        values = []\n",
    "        for action in ACTIONS:\n",
    "            next_state, reward = step(s, action)\n",
    "            values.append(reward + GAMMA * V[next_state])\n",
    "\n",
    "        new_V[s] = max(values)\n",
    "        delta = max(delta, abs(V[s] - new_V[s]))\n",
    "\n",
    "    V = new_V\n",
    "    iteration += 1\n",
    "\n",
    "    if delta < THRESHOLD:\n",
    "        break\n",
    "\n",
    "print(\"Converged in\", iteration, \"iterations\")\n",
    "print(\"\\nOptimal Value Function:\")\n",
    "print(np.round(V.reshape(GRID_SIZE, GRID_SIZE), 2))\n",
    "\n",
    "policy = {}\n",
    "for s in range(STATE_COUNT):\n",
    "    if s in TERMINAL_STATES:\n",
    "        policy[s] = None\n",
    "    else:\n",
    "        q_values = []\n",
    "        for action in ACTIONS:\n",
    "            next_state, reward = step(s, action)\n",
    "            q_values.append(reward + GAMMA * V[next_state])\n",
    "        best_action = ACTIONS[np.argmax(q_values)]\n",
    "        policy[s] = best_action\n",
    "\n",
    "print(\"\\nOptimal Policy (as arrows):\")\n",
    "policy_symbols = {\"U\": \"↑\", \"D\": \"↓\", \"L\": \"←\", \"R\": \"→\", None: \"T\"}\n",
    "\n",
    "output = []\n",
    "for s in range(STATE_COUNT):\n",
    "    output.append(policy_symbols[policy[s]])\n",
    "output = np.array(output).reshape(GRID_SIZE, GRID_SIZE)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b36f23-fab7-4536-8c3b-56b9e9c7d734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Value Function:\n",
      "  0.00  -1.00  -2.00  -3.00\n",
      " -1.00  -2.00  -3.00  -2.00\n",
      " -2.00  -3.00  -2.00  -1.00\n",
      " -3.00  -2.00  -1.00   0.00\n",
      "\n",
      "Optimal Policy (arrows):\n",
      "  T     ←     ←     ←  \n",
      "  ↑     ↑     ↑     ↓  \n",
      "  ↑     ↑     →     ↓  \n",
      "  ↑     →     →     T  \n",
      "\n",
      "Rollout from start (0,0):\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Simple MDP simulation in a 4x4 grid world for markov decision process (MDP) for Rollout the optimal policy from (0,0)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Gridworld parameters\n",
    "N_ROWS = 4\n",
    "N_COLS = 4\n",
    "N_STATES = N_ROWS * N_COLS\n",
    "ACTIONS = ['U', 'D', 'L', 'R']\n",
    "ACTION_DELTAS = {\n",
    "    'U': (-1, 0),\n",
    "    'D': (1, 0),\n",
    "    'L': (0, -1),\n",
    "    'R': (0, 1),\n",
    "}\n",
    "TERMINAL_STATES = [0, 15]\n",
    "\n",
    "ARROWS = {\n",
    "    'U': '↑',\n",
    "    'D': '↓',\n",
    "    'L': '←',\n",
    "    'R': '→',\n",
    "    'T': 'T'\n",
    "}\n",
    "\n",
    "def state_to_pos(s):\n",
    "    return divmod(s, N_COLS)\n",
    "\n",
    "def pos_to_state(r, c):\n",
    "    return r * N_COLS + c\n",
    "\n",
    "def step(state, action):\n",
    "    \"\"\"Returns next_state, reward.\"\"\"\n",
    "    if state in TERMINAL_STATES:\n",
    "        return state, 0\n",
    "\n",
    "    r, c = state_to_pos(state)\n",
    "    dr, dc = ACTION_DELTAS[action]\n",
    "    nr = min(max(r + dr, 0), N_ROWS - 1)\n",
    "    nc = min(max(c + dc, 0), N_COLS - 1)\n",
    "    next_state = pos_to_state(nr, nc)\n",
    "\n",
    "    reward = -1\n",
    "    return next_state, reward\n",
    "\n",
    "def value_iteration(gamma=1.0, theta=1e-4):\n",
    "    \"\"\"Runs Value Iteration and returns optimal V and optimal policy.\"\"\"\n",
    "    V = np.zeros(N_STATES)\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_V = np.copy(V)\n",
    "\n",
    "        for s in range(N_STATES):\n",
    "            if s in TERMINAL_STATES:\n",
    "                continue\n",
    "\n",
    "            q_values = []\n",
    "            for a in ACTIONS:\n",
    "                ns, r = step(s, a)\n",
    "                q_values.append(r + gamma * V[ns])\n",
    "\n",
    "            new_V[s] = max(q_values)\n",
    "            delta = max(delta, abs(new_V[s] - V[s]))\n",
    "\n",
    "        V = new_V\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Extract optimal policy\n",
    "    policy = np.empty(N_STATES, dtype=str)\n",
    "    for s in range(N_STATES):\n",
    "        if s in TERMINAL_STATES:\n",
    "            policy[s] = 'T'\n",
    "            continue\n",
    "\n",
    "        q_values = []\n",
    "        for a in ACTIONS:\n",
    "            ns, r = step(s, a)\n",
    "            q_values.append((r + gamma * V[ns], a))\n",
    "\n",
    "        policy[s] = max(q_values)[1]\n",
    "\n",
    "    return V, policy\n",
    "\n",
    "def rollout_policy(policy, start_state=0):\n",
    "    \"\"\"Simulates following the optimal policy from start_state.\"\"\"\n",
    "    state = start_state\n",
    "    trajectory = []\n",
    "\n",
    "    while True:\n",
    "        trajectory.append(state)\n",
    "        if state in TERMINAL_STATES:\n",
    "            break\n",
    "        action = policy[state]\n",
    "        next_state, reward = step(state, action)\n",
    "        state = next_state\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "def print_grid(values):\n",
    "    for r in range(N_ROWS):\n",
    "        row = values[r*N_COLS:(r+1)*N_COLS]\n",
    "        print(\" \".join(f\"{x:6.2f}\" if isinstance(x, (int,float)) else f\"  {x}  \" for x in row))\n",
    "    print()\n",
    "\n",
    "def print_policy_grid(policy):\n",
    "    arrow_policy = [ARROWS[p] for p in policy]\n",
    "    for r in range(N_ROWS):\n",
    "        row = arrow_policy[r*N_COLS:(r+1)*N_COLS]\n",
    "        print(\" \".join(f\"  {x}  \" for x in row))\n",
    "    print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Compute optimal V and policy\n",
    "    V, policy = value_iteration()\n",
    "\n",
    "    print(\"\\nOptimal Value Function:\")\n",
    "    print_grid(V)\n",
    "\n",
    "    print(\"Optimal Policy (arrows):\")\n",
    "    print_policy_grid(policy)\n",
    "\n",
    "    # Step 2: Rollout from (0,0)\n",
    "    print(\"Rollout from start (0,0):\")\n",
    "    trajectory = rollout_policy(policy, start_state=0)\n",
    "    print(\" → \".join(str(s) for s in trajectory))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43def83d-a17d-4d6f-b4c6-0b0d6522cdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learned Q-values:\n",
      "State 0: [0. 0. 0. 0.]\n",
      "State 1: [-1.98426336 -2.9595528  -1.         -2.95807319]\n",
      "State 2: [-2.91733892 -3.71013589 -1.99       -3.84070755]\n",
      "State 3: [-3.5059542 -2.9701    -2.9701    -3.4471014]\n",
      "State 4: [-1.         -2.95765275 -1.97673318 -2.96591216]\n",
      "State 5: [-1.99       -3.81556649 -1.99       -3.88200359]\n",
      "State 6: [-2.97009998 -2.97009998 -2.97009998 -2.97009998]\n",
      "State 7: [-3.82549926 -1.99       -3.85440009 -2.91770311]\n",
      "State 8: [-1.99       -3.73895776 -2.93186307 -3.7958451 ]\n",
      "State 9: [-2.97009995 -2.97009995 -2.97009995 -2.97009995]\n",
      "State 10: [-3.67820949 -1.99       -3.77207614 -1.99      ]\n",
      "State 11: [-2.96591764 -1.         -2.95972061 -1.98486613]\n",
      "State 12: [-2.9701     -3.51451074 -3.70538632 -2.9701    ]\n",
      "State 13: [-3.84813121 -2.88851547 -3.78080214 -1.99      ]\n",
      "State 14: [-2.96767107 -1.98937214 -2.96921778 -1.        ]\n",
      "State 15: [0. 0. 0. 0.]\n",
      "\n",
      "Learned Policy (arrows):\n",
      "  T     ←     ←     ←  \n",
      "  ↑     ↑     ←     ↓  \n",
      "  ↑     ←     ↓     ↓  \n",
      "  →     →     →     T  \n",
      "\n",
      "Rollout from start state (0,1):\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Simple MDP simulation in a 4x4 grid world for Markov Decision Process (MDP)\n",
    "# Learn from interaction with Q-learning (unknown model)\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Gridworld parameters\n",
    "N_ROWS = 4\n",
    "N_COLS = 4\n",
    "N_STATES = N_ROWS * N_COLS\n",
    "ACTIONS = ['U', 'D', 'L', 'R']\n",
    "ACTION_DELTAS = {\n",
    "    'U': (-1, 0),\n",
    "    'D': (1, 0),\n",
    "    'L': (0, -1),\n",
    "    'R': (0, 1),\n",
    "}\n",
    "TERMINAL_STATES = [0, 15]\n",
    "\n",
    "# Arrow symbols\n",
    "ARROWS = {\n",
    "    'U': '↑',\n",
    "    'D': '↓',\n",
    "    'L': '←',\n",
    "    'R': '→',\n",
    "    'T': 'T'\n",
    "}\n",
    "\n",
    "# --- Helpers ---\n",
    "def pos_to_state(r, c):\n",
    "    return r * N_COLS + c\n",
    "\n",
    "def state_to_pos(s):\n",
    "    return divmod(s, N_COLS)\n",
    "\n",
    "def step(state, action):\n",
    "    \"\"\"Environment transition: returns next_state, reward, done.\"\"\"\n",
    "    if state in TERMINAL_STATES:\n",
    "        return state, 0, True\n",
    "\n",
    "    r, c = state_to_pos(state)\n",
    "    dr, dc = ACTION_DELTAS[action]\n",
    "\n",
    "    nr = min(max(r + dr, 0), N_ROWS - 1)\n",
    "    nc = min(max(c + dc, 0), N_COLS - 1)\n",
    "    next_state = pos_to_state(nr, nc)\n",
    "\n",
    "    reward = -1\n",
    "    done = next_state in TERMINAL_STATES\n",
    "    return next_state, reward, done\n",
    "\n",
    "# --- Q-learning ---\n",
    "def q_learning(alpha=0.1, gamma=0.99, epsilon=0.1, episodes=5000, max_steps=100):\n",
    "    Q = np.zeros((N_STATES, len(ACTIONS)))\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state = random.choice([s for s in range(N_STATES) if s not in TERMINAL_STATES])\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            # ε-greedy selection\n",
    "            if random.random() < epsilon:\n",
    "                action_idx = random.randint(0, len(ACTIONS) - 1)\n",
    "            else:\n",
    "                action_idx = np.argmax(Q[state])\n",
    "\n",
    "            action = ACTIONS[action_idx]\n",
    "            next_state, reward, done = step(state, action)\n",
    "\n",
    "            # Q-learning update\n",
    "            td_target = reward + gamma * np.max(Q[next_state])\n",
    "            Q[state, action_idx] += alpha * (td_target - Q[state, action_idx])\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    return Q\n",
    "\n",
    "def extract_policy(Q):\n",
    "    \"\"\"Greedy policy using arrow symbols.\"\"\"\n",
    "    policy = []\n",
    "    for s in range(N_STATES):\n",
    "        if s in TERMINAL_STATES:\n",
    "            policy.append(ARROWS['T'])\n",
    "        else:\n",
    "            best_a = ACTIONS[np.argmax(Q[s])]\n",
    "            policy.append(ARROWS[best_a])\n",
    "    return policy\n",
    "\n",
    "def rollout(policy, start_state):\n",
    "    \"\"\"Rollout the learned policy.\"\"\"\n",
    "    state = start_state\n",
    "    traj = [state]\n",
    "\n",
    "    while state not in TERMINAL_STATES:\n",
    "        # Convert arrow back to action letter\n",
    "        arrow_to_action = {v: k for k, v in ARROWS.items()}\n",
    "        action = arrow_to_action[policy[state]]\n",
    "\n",
    "        next_state, reward, done = step(state, action)\n",
    "        traj.append(next_state)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return traj\n",
    "\n",
    "def print_grid(values):\n",
    "    for r in range(N_ROWS):\n",
    "        row = values[r*N_COLS:(r+1)*N_COLS]\n",
    "        print(\" \".join(f\"  {v}  \" for v in row))\n",
    "    print()\n",
    "\n",
    "# --- Run training ---\n",
    "if __name__ == \"__main__\":\n",
    "    Q = q_learning(episodes=8000)\n",
    "    policy = extract_policy(Q)\n",
    "\n",
    "    print(\"\\nLearned Q-values:\")\n",
    "    for s in range(N_STATES):\n",
    "        print(f\"State {s}: {Q[s]}\")\n",
    "\n",
    "    print(\"\\nLearned Policy (arrows):\")\n",
    "    print_grid(policy)\n",
    "\n",
    "    print(\"Rollout from start state (0,1):\")\n",
    "    print(rollout(policy, start_state=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91e3888e-56cf-4e53-86dd-e8faf2b18be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated State Values after TD(0):\n",
      "V(0) = 0.000\n",
      "V(1) = 0.099\n",
      "V(2) = 0.288\n",
      "V(3) = 0.410\n",
      "V(4) = 0.714\n",
      "V(5) = 0.888\n",
      "V(6) = 0.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#TD(0) Learning\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# -----------------------------\n",
    "# Simple Random Walk Environment\n",
    "# -----------------------------\n",
    "class RandomWalkEnv:\n",
    "    \"\"\"\n",
    "    States: 0 1 2 3 4 5 6\n",
    "    0 and 6 are terminal states.\n",
    "    Start state = 3 every episode.\n",
    "    Reward = +1 only when reaching state 6, else 0.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.start_state = 3\n",
    "        self.terminal_states = [0, 6]\n",
    "        self.state = self.start_state\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to start of episode.\"\"\"\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        action: -1 = move left, +1 = move right\n",
    "        returns: next_state, reward, done\n",
    "        \"\"\"\n",
    "        next_state = self.state + action\n",
    "\n",
    "        # Reward only if right terminal (state 6)\n",
    "        if next_state == 6:\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            reward = 0.0\n",
    "\n",
    "        self.state = next_state\n",
    "        done = next_state in self.terminal_states\n",
    "        return next_state, reward, done\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# TD(0) Algorithm (Prediction)\n",
    "# -----------------------------\n",
    "def td0_prediction(num_episodes=100, alpha=0.1, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Temporal Difference (TD(0)) Learning to estimate V(s).\n",
    "\n",
    "    num_episodes: how many episodes to run\n",
    "    alpha: learning rate\n",
    "    gamma: discount factor\n",
    "    \"\"\"\n",
    "    env = RandomWalkEnv()\n",
    "\n",
    "    # There are 7 states: 0 to 6\n",
    "    # Initialize value function V(s)\n",
    "    V = np.zeros(7)\n",
    "\n",
    "    # Terminal states are fixed (by definition of the problem)\n",
    "    V[0] = 0.0\n",
    "    V[6] = 0.0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        while True:\n",
    "            # Policy: choose left or right with equal probability\n",
    "            action = random.choice([-1, 1])\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            # TD(0) Update:\n",
    "            # V(s) ← V(s) + α [ r + γ V(s') - V(s) ]\n",
    "            td_target = reward + gamma * V[next_state]\n",
    "            td_error = td_target - V[state]\n",
    "            V[state] = V[state] + alpha * td_error\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run TD(0)\n",
    "    num_episodes = 200  # you can increase for better estimates\n",
    "    alpha = 0.1\n",
    "    gamma = 1.0\n",
    "\n",
    "    V = td0_prediction(num_episodes=num_episodes, alpha=alpha, gamma=gamma)\n",
    "\n",
    "    print(\"Estimated State Values after TD(0):\")\n",
    "    for s in range(7):\n",
    "        print(f\"V({s}) = {V[s]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47c7d0a8-fd48-4956-a045-7e60ca0ed1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated State Values after TD(λ):\n",
      "V(0) = 0.000\n",
      "V(1) = 0.108\n",
      "V(2) = 0.362\n",
      "V(3) = 0.477\n",
      "V(4) = 0.772\n",
      "V(5) = 0.929\n",
      "V(6) = 0.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#TD(lambda)Learning\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# -----------------------------\n",
    "# Simple Random Walk Environment\n",
    "# -----------------------------\n",
    "class RandomWalkEnv:\n",
    "    \"\"\"\n",
    "    States: 0 1 2 3 4 5 6\n",
    "    0 and 6 are terminal states.\n",
    "    Start state = 3 every episode.\n",
    "    Reward = +1 only when reaching state 6, else 0.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.start_state = 3\n",
    "        self.terminal_states = [0, 6]\n",
    "        self.state = self.start_state\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to start of episode.\"\"\"\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        action: -1 = move left, +1 = move right\n",
    "        returns: next_state, reward, done\n",
    "        \"\"\"\n",
    "        next_state = self.state + action\n",
    "\n",
    "        # Reward only if right terminal (state 6)\n",
    "        if next_state == 6:\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            reward = 0.0\n",
    "\n",
    "        self.state = next_state\n",
    "        done = next_state in self.terminal_states\n",
    "        return next_state, reward, done\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# TD(λ) Algorithm (Prediction)\n",
    "# -----------------------------\n",
    "def td_lambda_prediction(num_episodes=100, alpha=0.1, gamma=1.0, lam=0.8):\n",
    "    \"\"\"\n",
    "    Temporal Difference TD(λ) Learning to estimate V(s) using eligibility traces.\n",
    "\n",
    "    num_episodes: number of episodes\n",
    "    alpha       : learning rate\n",
    "    gamma       : discount factor\n",
    "    lam         : lambda parameter (0 <= λ <= 1)\n",
    "    \"\"\"\n",
    "    env = RandomWalkEnv()\n",
    "\n",
    "    # There are 7 states: 0 to 6\n",
    "    V = np.zeros(7)\n",
    "\n",
    "    # terminal values\n",
    "    V[0] = 0.0\n",
    "    V[6] = 0.0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        # Initialize eligibility traces to 0 for all states\n",
    "        E = np.zeros(7)\n",
    "\n",
    "        while True:\n",
    "            # Behaviour policy: random left/right\n",
    "            action = random.choice([-1, 1])\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            # TD error δ = r + γ V(s') − V(s)\n",
    "            td_target = reward + gamma * V[next_state]\n",
    "            td_error = td_target - V[state]\n",
    "\n",
    "            # Increase eligibility of current state\n",
    "            E[state] += 1.0     # accumulating traces\n",
    "\n",
    "            # Update all states' values using their eligibility\n",
    "            V += alpha * td_error * E\n",
    "\n",
    "            # Decay eligibility traces\n",
    "            E *= gamma * lam\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_episodes = 200\n",
    "    alpha = 0.1\n",
    "    gamma = 1.0\n",
    "    lam = 0.8\n",
    "\n",
    "    V = td_lambda_prediction(num_episodes=num_episodes,\n",
    "                             alpha=alpha, gamma=gamma, lam=lam)\n",
    "\n",
    "    print(\"Estimated State Values after TD(λ):\")\n",
    "    for s in range(7):\n",
    "        print(f\"V({s}) = {V[s]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd0d6d36-5bda-480d-bc75-6049ad997d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values:\n",
      " [[ 0.          0.          0.          0.        ]\n",
      " [-1.         -2.75969433 -1.93825794 -2.64678806]\n",
      " [-2.05736249 -3.27416015 -2.51254639 -3.21308758]\n",
      " [-2.77978564 -3.3441002  -3.26550571 -3.07328088]\n",
      " [-2.01571618 -2.74830145 -1.         -2.84609271]\n",
      " [-1.90439482 -3.26326488 -2.16444694 -3.31811676]\n",
      " [-2.8802241  -2.85954204 -2.89831784 -2.92199652]\n",
      " [-3.15908061 -2.63701824 -3.45466874 -1.9265822 ]\n",
      " [-2.62791552 -3.1930961  -1.91237278 -3.16110425]\n",
      " [-2.91718837 -2.94894018 -2.79835264 -2.9226367 ]\n",
      " [-3.05521843 -2.12749049 -3.3840957  -1.91455912]\n",
      " [-2.70340466 -1.96902541 -2.90034408 -1.        ]\n",
      " [-3.16344124 -2.80676776 -2.92140916 -3.16811076]\n",
      " [-3.08968346 -1.92454024 -3.2422792  -2.71800099]\n",
      " [-2.79405122 -1.         -2.71554707 -1.84246869]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "\n",
      "Optimal Policy (0=L,1=R,2=U,3=D):\n",
      "[[0 0 0 0]\n",
      " [2 0 1 3]\n",
      " [2 2 3 3]\n",
      " [1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#**7.Implement SARSA**\n",
    "import numpy as np\n",
    "np.random.seed(3)\n",
    "\n",
    "terminal=[0,15]\n",
    "actions=[-1,1,-4,4]\n",
    "\n",
    "def step(s,a):\n",
    "    ns=s+a\n",
    "    if s%4==0 and a==-1: ns=s\n",
    "    if s%4==3 and a==1:  ns=s\n",
    "    if s<4 and a==-4:    ns=s\n",
    "    if s>11 and a==4:    ns=s\n",
    "    return ns, -1\n",
    "\n",
    "Q = np.zeros((16,4))\n",
    "alpha=0.1; gamma=0.9; eps=0.1\n",
    "\n",
    "def choose_action(s):\n",
    "    return np.random.randint(4) if np.random.rand()<eps else np.argmax(Q[s])\n",
    "\n",
    "for ep in range(5000):\n",
    "    s=np.random.randint(1,15)\n",
    "    a=choose_action(s)\n",
    "    while s not in terminal:\n",
    "        ns,r=step(s, actions[a])\n",
    "        na=choose_action(ns)\n",
    "        Q[s,a]+=alpha*(r + gamma*Q[ns,na] - Q[s,a])\n",
    "        s, a = ns, na\n",
    "\n",
    "print(\"Q-values:\\n\",Q)\n",
    "print(\"\\nOptimal Policy (0=L,1=R,2=U,3=D):\")\n",
    "print(np.argmax(Q,axis=1).reshape(4,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc208190-b6bb-4181-aacf-ff5de4159212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table:\n",
      " [[ 0.          0.          0.          0.        ]\n",
      " [-1.         -2.65063947 -1.88072721 -2.68025455]\n",
      " [-1.9        -2.91640453 -2.48169662 -3.20758561]\n",
      " [-2.71       -3.05913846 -3.1965142  -2.71      ]\n",
      " [-1.8760517  -2.65954214 -1.         -2.64150682]\n",
      " [-1.9        -3.10434146 -1.9        -3.26946587]\n",
      " [-2.70998897 -2.70998868 -2.70998868 -2.70998801]\n",
      " [-3.17904553 -2.48040655 -3.06180302 -1.9       ]\n",
      " [-2.5744077  -3.25902145 -1.9        -3.05404097]\n",
      " [-2.70998301 -2.70998334 -2.70998215 -2.70998187]\n",
      " [-3.136205   -1.9        -2.88301612 -1.9       ]\n",
      " [-2.67518004 -1.88586939 -2.58655912 -1.        ]\n",
      " [-3.18966072 -2.71       -2.71       -3.19693897]\n",
      " [-3.15297499 -1.9        -3.09542527 -2.55924191]\n",
      " [-2.60924688 -1.         -2.68022988 -1.86335926]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "\n",
      "Optimal Policy:\n",
      "[[0 0 0 0]\n",
      " [2 0 3 3]\n",
      " [2 3 1 3]\n",
      " [2 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# **8.Implement Q Learning**\n",
    "import numpy as np\n",
    "np.random.seed(5)\n",
    "\n",
    "terminal=[0,15]\n",
    "actions=[-1,1,-4,4]\n",
    "\n",
    "def step(s,a):\n",
    "    ns=s+a\n",
    "    if s%4==0 and a==-1: ns=s\n",
    "    if s%4==3 and a==1:  ns=s\n",
    "    if s<4 and a==-4:    ns=s\n",
    "    if s>11 and a==4:    ns=s\n",
    "    return ns, -1\n",
    "\n",
    "Q=np.zeros((16,4))\n",
    "alpha=0.1; gamma=0.9; eps=0.1\n",
    "\n",
    "def choose(s):\n",
    "    return np.random.randint(4) if np.random.rand()<eps else np.argmax(Q[s])\n",
    "\n",
    "for ep in range(5000):\n",
    "    s=np.random.randint(1,15)\n",
    "    while s not in terminal:\n",
    "        a=choose(s)\n",
    "        ns,r=step(s, actions[a])\n",
    "        Q[s,a]+=alpha*(r + gamma*np.max(Q[ns]) - Q[s,a])\n",
    "        s=ns\n",
    "\n",
    "print(\"Q-table:\\n\",Q)\n",
    "print(\"\\nOptimal Policy:\")\n",
    "print(np.argmax(Q,axis=1).reshape(4,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37eb371-4e87-490e-be42-e902542578e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08183aa4-963f-4712-855e-b36ef275bad2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mgymnasium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgym\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "#DQN\n",
    "import random, collections, math, numpy as np, gymnasium as gym, torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Simple Q-Network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, n_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 128), nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Replay buffer\n",
    "Transition = collections.namedtuple(\"Transition\", (\"s\", \"a\", \"r\", \"s2\", \"done\"))\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "    def push(self, *args): self.buffer.append(Transition(*args))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        return Transition(*zip(*batch))\n",
    "    def __len__(self): return len(self.buffer)\n",
    "\n",
    "# DQN training function\n",
    "def train_dqn():\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    policy_net = QNetwork(obs_dim, n_actions).to(device)\n",
    "    target_net = QNetwork(obs_dim, n_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "    buffer = ReplayBuffer()\n",
    "    GAMMA, BATCH_SIZE, TARGET_UPDATE, EPS_DECAY = 0.99, 64, 500, 5000\n",
    "    EPS_START, EPS_END = 1.0, 0.01\n",
    "    total_steps, rewards = 0, []\n",
    "\n",
    "    def epsilon(step):\n",
    "        return EPS_END + (EPS_START - EPS_END) * math.exp(-1. * step / EPS_DECAY)\n",
    "\n",
    "    for ep in range(300):\n",
    "        state, _ = env.reset()\n",
    "        done, ep_reward = False, 0\n",
    "        while not done:\n",
    "            eps = epsilon(total_steps)\n",
    "            total_steps += 1\n",
    "            if random.random() < eps:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    s = torch.tensor(np.array(state), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                    action = int(policy_net(s).argmax(1).item())\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "\n",
    "            if len(buffer) >= BATCH_SIZE:\n",
    "                transitions = buffer.sample(BATCH_SIZE)\n",
    "                s = torch.tensor(np.array(transitions.s), dtype=torch.float32, device=device)\n",
    "                a = torch.tensor(transitions.a, dtype=torch.int64, device=device).unsqueeze(1)\n",
    "                r = torch.tensor(transitions.r, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "                s2 = torch.tensor(np.array(transitions.s2), dtype=torch.float32, device=device)\n",
    "                done_mask = torch.tensor(transitions.done, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "                q_vals = policy_net(s).gather(1, a)\n",
    "                with torch.no_grad():\n",
    "                    q_next = target_net(s2).max(1)[0].unsqueeze(1)\n",
    "                    q_target = r + GAMMA * q_next * (1 - done_mask)\n",
    "\n",
    "                loss = nn.functional.mse_loss(q_vals, q_target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if total_steps % TARGET_UPDATE == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        rewards.append(ep_reward)\n",
    "        if ep % 10 == 0:\n",
    "            print(f\"Episode {ep} | Avg Reward: {np.mean(rewards[-10:]):.2f}\")\n",
    "\n",
    "    env.close()\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"DQN Training (Gymnasium - CartPole)\")\n",
    "    plt.show()\n",
    "\n",
    "train_dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3707b737-39e9-480b-b364-7ed3498e49eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Implement REINFORCE algorithm with Baseline\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Implement REINFORCE algorithm with Baseline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------\n",
    "#  Policy Network (Actor)\n",
    "# ---------------------------------------------\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def _init_(self, state_dim, action_dim):\n",
    "        super()._init_()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# ---------------------------------------------\n",
    "#  Baseline Network (State Value)\n",
    "# ---------------------------------------------\n",
    "class ValueNetwork(nn.Module):\n",
    "    def _init_(self, state_dim):\n",
    "        super()._init_()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# ---------------------------------------------\n",
    "#  Compute discounted returns\n",
    "# ---------------------------------------------\n",
    "def compute_returns(rewards, gamma=0.99):\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "# ---------------------------------------------\n",
    "#  REINFORCE with Baseline – Training Loop\n",
    "# ---------------------------------------------\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "policy_net = PolicyNetwork(state_dim, action_dim)\n",
    "value_net = ValueNetwork(state_dim)\n",
    "\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "value_optimizer = optim.Adam(value_net.parameters(), lr=1e-3)\n",
    "\n",
    "num_episodes = 100\n",
    "gamma = 0.99\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    log_probs, values, rewards = [], [], []\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        probs = policy_net(state_tensor)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        log_probs.append(dist.log_prob(action))\n",
    "        values.append(value_net(state_tensor))\n",
    "        rewards.append(reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Compute returns\n",
    "    returns = compute_returns(rewards, gamma)\n",
    "    values = torch.cat(values).squeeze()\n",
    "\n",
    "    # Baseline is value function (detached)\n",
    "    baseline = values.detach()\n",
    "\n",
    "    # Advantage = Return - Baseline\n",
    "    advantages = returns - baseline\n",
    "\n",
    "    # Policy loss\n",
    "    policy_loss = -(torch.stack(log_probs) * advantages).mean()\n",
    "\n",
    "    # Value loss = MSE(Returns, Baseline)\n",
    "    value_loss = (returns - values).pow(2).mean()\n",
    "\n",
    "    # Update policy\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "\n",
    "    # Update value baseline\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    value_optimizer.step()\n",
    "\n",
    "    print(f\"Episode {episode+1}: Reward = {sum(rewards)}\")\n",
    "\n",
    "env.close()\n",
    "print(\"REINFORCE with baseline training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3280d579-c02d-423e-abc2-fc9f3482423d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Implement REINFORCE algorithm with Advantage Function.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Implement REINFORCE algorithm with Advantage Function.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------\n",
    "#  Policy Network (Actor)\n",
    "# ---------------------------------------------\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def _init_(self, state_dim, action_dim):\n",
    "        super()._init_()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# ---------------------------------------------\n",
    "#  Value Network (Critic)\n",
    "# ---------------------------------------------\n",
    "class ValueNetwork(nn.Module):\n",
    "    def _init_(self, state_dim):\n",
    "        super()._init_()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# ---------------------------------------------\n",
    "#  Compute discounted returns\n",
    "# ---------------------------------------------\n",
    "def compute_returns(rewards, gamma=0.99):\n",
    "    R = 0\n",
    "    returns = []\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "# ---------------------------------------------\n",
    "#  REINFORCE with Advantage – Training Loop\n",
    "# ---------------------------------------------\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "policy_net = PolicyNetwork(state_dim, action_dim)\n",
    "value_net = ValueNetwork(state_dim)\n",
    "\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "value_optimizer = optim.Adam(value_net.parameters(), lr=1e-3)\n",
    "\n",
    "num_episodes = 100\n",
    "gamma = 0.99\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    log_probs, values, rewards = [], [], []\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        probs = policy_net(state_tensor)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        log_probs.append(dist.log_prob(action))\n",
    "        values.append(value_net(state_tensor))\n",
    "        rewards.append(reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Compute returns\n",
    "    returns = compute_returns(rewards, gamma)\n",
    "    values = torch.cat(values).squeeze()\n",
    "\n",
    "    # ADVANTAGE = Return − Value Estimate\n",
    "    advantages = returns - values.detach()\n",
    "\n",
    "    # POLICY LOSS (Actor)\n",
    "    policy_loss = -(torch.stack(log_probs) * advantages).mean()\n",
    "\n",
    "    # VALUE LOSS (Critic)\n",
    "    value_loss = (returns - values).pow(2).mean()\n",
    "\n",
    "    # Update policy\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "\n",
    "    # Update critic\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    value_optimizer.step()\n",
    "\n",
    "    print(f\"Episode {episode+1}: Reward = {sum(rewards)}\")\n",
    "\n",
    "env.close()\n",
    "print(\"REINFORCE with advantage training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6859a19a-6070-4121-b3df-8e0535d116fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (arrows):\n",
      "[['T' '←' '←' '↓']\n",
      " ['↑' '←' '←' '↓']\n",
      " ['↑' '↓' '↑' '↓']\n",
      " ['↑' '←' '→' 'T']]\n",
      "\n",
      "Q-table:\n",
      "[[  0.           0.           0.           0.        ]\n",
      " [ -1.          -8.35714286  -5.72       -14.5       ]\n",
      " [ -2.46614583 -13.57142857 -18.36363636 -13.3       ]\n",
      " [-16.61904762  -8.93333333  -8.4         -3.64050633]\n",
      " [ -2.31460674  -3.47191011  -1.          -3.56521739]\n",
      " [ -2.17818182  -7.90625     -7.86206897 -11.60526316]\n",
      " [ -3.50831025  -9.21052632  -7.20833333 -10.85714286]\n",
      " [ -8.03030303  -9.36       -12.8         -2.37075718]\n",
      " [ -3.54054054  -6.64583333  -2.16372283  -9.85365854]\n",
      " [ -9.5625     -14.5        -18.33333333  -5.87259615]\n",
      " [-23.27272727 -18.85714286  -4.32533333  -7.66666667]\n",
      " [ -9.17857143  -2.34210526  -6.55        -1.        ]\n",
      " [-13.27272727 -12.13888889  -3.58212996 -13.2       ]\n",
      " [ -4.61216216  -9.         -12.39130435  -9.78947368]\n",
      " [-20.71428571  -1.         -12.8        -20.9       ]\n",
      " [  0.           0.           0.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# **12. Implement the Monte Carlo prediction and control.**\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "actions = [-1, 1, -4, 4]         # L, R, U, D\n",
    "terminal = [0, 15]\n",
    "\n",
    "# Arrow mapping\n",
    "ARROWS = {\n",
    "    -1: \"←\",\n",
    "     1: \"→\",\n",
    "    -4: \"↑\",\n",
    "     4: \"↓\",\n",
    "    \"T\": \"T\"\n",
    "}\n",
    "\n",
    "def step(s,a):\n",
    "    ns = s + a\n",
    "    if s % 4 == 0 and a == -1: ns = s\n",
    "    if s % 4 == 3 and a == 1:  ns = s\n",
    "    if s < 4 and a == -4:      ns = s\n",
    "    if s > 11 and a == 4:      ns = s\n",
    "    return ns, -1\n",
    "\n",
    "Q = np.zeros((16,4))\n",
    "returns = { (s,a):[] for s in range(16) for a in range(4) }\n",
    "eps = 0.1\n",
    "gamma = 1.0\n",
    "\n",
    "def choose(s):   # ε-greedy\n",
    "    if np.random.rand() < eps:\n",
    "        return np.random.randint(4)\n",
    "    return np.argmax(Q[s])\n",
    "\n",
    "# Monte Carlo Control\n",
    "for ep in range(5000):\n",
    "    s = np.random.randint(1,15)\n",
    "    episode = []\n",
    "\n",
    "    while s not in terminal:\n",
    "        a = choose(s)\n",
    "        ns, r = step(s, actions[a])\n",
    "        episode.append((s,a,r))\n",
    "        s = ns\n",
    "\n",
    "    G = 0\n",
    "    for i in reversed(range(len(episode))):\n",
    "        s, a, r = episode[i]\n",
    "        G += r\n",
    "\n",
    "        # First-visit check\n",
    "        if not any(s == x[0] and a == x[1] for x in episode[:i]):\n",
    "            returns[(s,a)].append(G)\n",
    "            Q[s,a] = np.mean(returns[(s,a)])\n",
    "\n",
    "# Greedy policy\n",
    "policy_idx = np.argmax(Q, axis=1)\n",
    "\n",
    "# Convert to arrows\n",
    "arrow_policy = []\n",
    "for s in range(16):\n",
    "    if s in terminal:\n",
    "        arrow_policy.append(ARROWS[\"T\"])\n",
    "    else:\n",
    "        arr = ARROWS[ actions[ policy_idx[s] ] ]\n",
    "        arrow_policy.append(arr)\n",
    "\n",
    "arrow_policy = np.array(arrow_policy).reshape(4,4)\n",
    "\n",
    "print(\"Optimal Policy (arrows):\")\n",
    "print(arrow_policy)\n",
    "\n",
    "print(\"\\nQ-table:\")\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7aecfd5-18f1-4132-bd04-aa1766964e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weights w: [  0.11440153   0.15696317 -14.33689722]\n",
      "\n",
      "Approximated Value Function:\n",
      "[[-14.33689722 -14.17993405 -14.02297088 -13.86600771]\n",
      " [-14.22249568 -14.06553252 -13.90856935 -13.75160618]\n",
      " [-14.10809415 -13.95113098 -13.79416782 -13.63720465]\n",
      " [-13.99369262 -13.83672945 -13.67976628 -13.52280312]]\n"
     ]
    }
   ],
   "source": [
    "#Q13: To implement function approximation using linear model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Gridworld\n",
    "N_ROWS, N_COLS = 4, 4\n",
    "TERMINAL = [0, 15]\n",
    "\n",
    "# Step function\n",
    "def step(s, a):\n",
    "    r, c = divmod(s, N_COLS)\n",
    "    dr, dc = {0:(-1,0), 1:(1,0), 2:(0,-1), 3:(0,1)}[a]  # U,D,L,R\n",
    "    nr, nc = min(max(r + dr, 0), N_ROWS-1), min(max(c + dc, 0), N_COLS-1)\n",
    "    ns = nr * N_COLS + nc\n",
    "    reward = -1\n",
    "    return ns, reward\n",
    "\n",
    "# --- Features ---\n",
    "def features(s):\n",
    "    r, c = divmod(s, N_COLS)\n",
    "    return np.array([r, c, 1.0])   # simple 3-dimensional features\n",
    "\n",
    "# --- Linear Value Function ---\n",
    "def V(s, w):\n",
    "    return np.dot(w, features(s))\n",
    "\n",
    "# --- TD(0) with function approximation ---\n",
    "def td_linear(alpha=0.01, gamma=0.99, episodes=1000):\n",
    "    w = np.zeros(3)  # weights for the linear model\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        s = np.random.randint(1, 15)   # start non-terminal\n",
    "\n",
    "        while s not in TERMINAL:\n",
    "            # choose a random action for prediction-only\n",
    "            a = np.random.choice([0,1,2,3])\n",
    "\n",
    "            ns, r = step(s, a)\n",
    "\n",
    "            # TD target\n",
    "            td_target = r + gamma * (0 if ns in TERMINAL else V(ns, w))\n",
    "            td_error = td_target - V(s, w)\n",
    "\n",
    "            # gradient update: w += alpha * error * x(s)\n",
    "            w += alpha * td_error * features(s)\n",
    "\n",
    "            s = ns\n",
    "\n",
    "    return w\n",
    "\n",
    "# --- Run ---\n",
    "w = td_linear()\n",
    "print(\"Learned weights w:\", w)\n",
    "\n",
    "# Value table from approximator\n",
    "value_table = np.array([V(s, w) for s in range(16)]).reshape(4,4)\n",
    "print(\"\\nApproximated Value Function:\")\n",
    "print(value_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a8722-5480-46b6-a4c0-7489c3807a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
